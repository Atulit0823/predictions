{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atuli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DecompositionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Returns the trend and the seasonal parts of the time series.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=1, padding=0) # moving average\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Input shape: Batch x Time x EMBED_DIM\"\"\"\n",
    "        # padding on the both ends of time series\n",
    "        num_of_pads = (self.kernel_size - 1) // 2\n",
    "        front = x[:, 0:1, :].repeat(1, num_of_pads, 1)\n",
    "        end = x[:, -1:, :].repeat(1, num_of_pads, 1)\n",
    "        x_padded = torch.cat([front, x, end], dim=1)\n",
    "\n",
    "        # calculate the trend and seasonal part of the series\n",
    "        x_trend = self.avg(x_padded.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x_seasonal = x - x_trend\n",
    "        return x_seasonal, x_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def autocorrelation(query_states, key_states):\n",
    "    \"\"\"\n",
    "    Computes autocorrelation(Q,K) using `torch.fft`.\n",
    "    Think about it as a replacement for the QK^T in the self-attention.\n",
    "\n",
    "    Assumption: states are resized to same shape of [batch_size, time_length, embedding_dim].\n",
    "    \"\"\"\n",
    "    query_states_fft = torch.fft.rfft(query_states, dim=1)\n",
    "    key_states_fft = torch.fft.rfft(key_states, dim=1)\n",
    "    attn_weights = query_states_fft * torch.conj(key_states_fft)\n",
    "    attn_weights = torch.fft.irfft(attn_weights, dim=1)\n",
    "\n",
    "    return attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def time_delay_aggregation(attn_weights, value_states, autocorrelation_factor=2):\n",
    "    \"\"\"\n",
    "    Computes aggregation as value_states.roll(delay) * top_k_autocorrelations(delay).\n",
    "    The final result is the autocorrelation-attention output.\n",
    "    Think about it as a replacement of the dot-product between attn_weights and value states.\n",
    "\n",
    "    The autocorrelation_factor is used to find top k autocorrelations delays.\n",
    "    Assumption: value_states and attn_weights shape: [batch_size, time_length, embedding_dim]\n",
    "    \"\"\"\n",
    "    bsz, num_heads, tgt_len, channel = ...\n",
    "    time_length = value_states.size(1)\n",
    "    autocorrelations = attn_weights.view(bsz, num_heads, tgt_len, channel)\n",
    "\n",
    "    top_k = int(autocorrelation_factor * math.log(time_length))\n",
    "    autocorrelations_mean = torch.mean(autocorrelations, dim=(1, -1))  # bsz x tgt_len\n",
    "    top_k_autocorrelations, top_k_delays = torch.topk(autocorrelations_mean, top_k, dim=1)\n",
    "\n",
    "    top_k_autocorrelations = torch.softmax(top_k_autocorrelations, dim=-1)  \n",
    "\n",
    "    delays_agg = torch.zeros_like(value_states).float()  \n",
    "    for i in range(top_k):\n",
    "        value_states_roll_delay = value_states.roll(shifts=-int(top_k_delays[i]), dims=1)\n",
    "        top_k_at_delay = top_k_autocorrelations[:, i]\n",
    "        top_k_resized = top_k_at_delay.view(-1, 1, 1).repeat(num_heads, tgt_len, channel)\n",
    "        delays_agg += value_states_roll_delay * top_k_resized\n",
    "\n",
    "    attn_output = delays_agg.contiguous()\n",
    "    return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 505 entries, 0 to 504\n",
      "Data columns (total 6 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   TimeReviewed                 505 non-null    datetime64[ns]\n",
      " 1   Solar_Power_Consumption(Kw)  505 non-null    float64       \n",
      " 2   Temp( C)                     505 non-null    float64       \n",
      " 3   EmployeeCount                505 non-null    int64         \n",
      " 4   weekday                      505 non-null    int32         \n",
      " 5   day_type                     505 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(3), int32(1), int64(1)\n",
      "memory usage: 21.8 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atuli\\AppData\\Local\\Temp\\ipykernel_13104\\1367160064.py:5: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  dailyEnergy['day_type'][(dailyEnergy['weekday']==5)|(dailyEnergy['weekday']==6)|(dailyEnergy['TimeReviewed']=='2017-12-26')|(dailyEnergy['TimeReviewed']=='2018-1-1')|(dailyEnergy['TimeReviewed']=='2018-1-14')|(dailyEnergy['TimeReviewed']=='2018-1-26')|(dailyEnergy['TimeReviewed']=='2018-5-1')|(dailyEnergy['TimeReviewed']=='2018-8-15')|(dailyEnergy['TimeReviewed']=='2018-10-2')|(dailyEnergy['TimeReviewed']=='2018-12-25')|(dailyEnergy['TimeReviewed']=='2019-1-1')|(dailyEnergy['TimeReviewed']=='2019-1-14')|(dailyEnergy['TimeReviewed']=='2019-1-26')|(dailyEnergy['TimeReviewed']=='2019-5-1')]=1\n",
      "C:\\Users\\atuli\\AppData\\Local\\Temp\\ipykernel_13104\\1367160064.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dailyEnergy['day_type'][(dailyEnergy['weekday']==5)|(dailyEnergy['weekday']==6)|(dailyEnergy['TimeReviewed']=='2017-12-26')|(dailyEnergy['TimeReviewed']=='2018-1-1')|(dailyEnergy['TimeReviewed']=='2018-1-14')|(dailyEnergy['TimeReviewed']=='2018-1-26')|(dailyEnergy['TimeReviewed']=='2018-5-1')|(dailyEnergy['TimeReviewed']=='2018-8-15')|(dailyEnergy['TimeReviewed']=='2018-10-2')|(dailyEnergy['TimeReviewed']=='2018-12-25')|(dailyEnergy['TimeReviewed']=='2019-1-1')|(dailyEnergy['TimeReviewed']=='2019-1-14')|(dailyEnergy['TimeReviewed']=='2019-1-26')|(dailyEnergy['TimeReviewed']=='2019-5-1')]=1\n"
     ]
    }
   ],
   "source": [
    "dailyEnergy = pd.read_excel('dailyEnergyWithFeatures.xlsx')\n",
    "dailyEnergy['weekday'] = dailyEnergy['TimeReviewed'].dt.dayofweek\n",
    "dailyEnergy['day_type'] = np.zeros(len(dailyEnergy))\n",
    "\n",
    "dailyEnergy['day_type'][(dailyEnergy['weekday']==5)|(dailyEnergy['weekday']==6)|(dailyEnergy['TimeReviewed']=='2017-12-26')|(dailyEnergy['TimeReviewed']=='2018-1-1')|(dailyEnergy['TimeReviewed']=='2018-1-14')|(dailyEnergy['TimeReviewed']=='2018-1-26')|(dailyEnergy['TimeReviewed']=='2018-5-1')|(dailyEnergy['TimeReviewed']=='2018-8-15')|(dailyEnergy['TimeReviewed']=='2018-10-2')|(dailyEnergy['TimeReviewed']=='2018-12-25')|(dailyEnergy['TimeReviewed']=='2019-1-1')|(dailyEnergy['TimeReviewed']=='2019-1-14')|(dailyEnergy['TimeReviewed']=='2019-1-26')|(dailyEnergy['TimeReviewed']=='2019-5-1')]=1\n",
    "dailyEnergy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeReviewed</th>\n",
       "      <th>Solar_Power_Consumption(Kw)</th>\n",
       "      <th>Temp( C)</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>weekday</th>\n",
       "      <th>day_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>12782.411988</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>13883.010020</td>\n",
       "      <td>25.375000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-28</td>\n",
       "      <td>13766.083029</td>\n",
       "      <td>24.833333</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>12904.092990</td>\n",
       "      <td>24.833333</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>7985.972000</td>\n",
       "      <td>25.906250</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2019-05-10</td>\n",
       "      <td>15619.215017</td>\n",
       "      <td>27.041667</td>\n",
       "      <td>184</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>2019-05-11</td>\n",
       "      <td>9179.519970</td>\n",
       "      <td>26.662921</td>\n",
       "      <td>166</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>2019-05-12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>12396.665989</td>\n",
       "      <td>27.169492</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>6716.024996</td>\n",
       "      <td>27.795918</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    TimeReviewed  Solar_Power_Consumption(Kw)   Temp( C)  EmployeeCount  \\\n",
       "0     2017-12-26                 12782.411988  25.000000            121   \n",
       "1     2017-12-27                 13883.010020  25.375000              0   \n",
       "2     2017-12-28                 13766.083029  24.833333              0   \n",
       "3     2017-12-29                 12904.092990  24.833333              0   \n",
       "4     2017-12-30                  7985.972000  25.906250              0   \n",
       "..           ...                          ...        ...            ...   \n",
       "500   2019-05-10                 15619.215017  27.041667            184   \n",
       "501   2019-05-11                  9179.519970  26.662921            166   \n",
       "502   2019-05-12                     0.000000  29.000000             11   \n",
       "503   2019-05-13                 12396.665989  27.169492            186   \n",
       "504   2019-05-14                  6716.024996  27.795918            193   \n",
       "\n",
       "     weekday  day_type  \n",
       "0          1       1.0  \n",
       "1          2       0.0  \n",
       "2          3       0.0  \n",
       "3          4       0.0  \n",
       "4          5       1.0  \n",
       "..       ...       ...  \n",
       "500        4       0.0  \n",
       "501        5       1.0  \n",
       "502        6       1.0  \n",
       "503        0       0.0  \n",
       "504        1       0.0  \n",
       "\n",
       "[505 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dailyEnergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elect = dailyEnergy[[\"Temp( C)\", \"EmployeeCount\", \"weekday\",\"day_type\", 'Solar_Power_Consumption(Kw)']]\n",
    "\n",
    "elect_train = pd.DataFrame(data=df_elect.head(350))\n",
    "elect_test = pd.DataFrame(data=df_elect.head(155))\n",
    "\n",
    "XX_elect_train = elect_train.drop('Solar_Power_Consumption(Kw)', axis = 1).reset_index().drop('index', axis = 1)\n",
    "XX_elect_test = elect_test.drop('Solar_Power_Consumption(Kw)', axis = 1).reset_index().drop('index', axis = 1)\n",
    "\n",
    "YY_elect_train = elect_train[['Solar_Power_Consumption(Kw)']]\n",
    "YY_elect_test = elect_test[['Solar_Power_Consumption(Kw)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(YY_elect_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Solar_Power_Consumption(Kw)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12782.411988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13883.010020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13766.083029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12904.092990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7985.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>9587.865992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>9994.493010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>8061.357007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>997.568998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>9882.566991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Solar_Power_Consumption(Kw)\n",
       "0                   12782.411988\n",
       "1                   13883.010020\n",
       "2                   13766.083029\n",
       "3                   12904.092990\n",
       "4                    7985.972000\n",
       "..                           ...\n",
       "345                  9587.865992\n",
       "346                  9994.493010\n",
       "347                  8061.357007\n",
       "348                   997.568998\n",
       "349                  9882.566991\n",
       "\n",
       "[350 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YY_elect_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atuli\\AppData\\Local\\Temp\\ipykernel_13104\\3633244058.py:24: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
      "  dailyEnergy.loc[dailyEnergy['TimeReviewed'].isin(holidays), 'day_type'] = 1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 67\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m#         loss = loss_fn(output.squeeze(), y_batch)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m#         total_loss += loss.item()\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m#         loss.backward()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# print(f\"Test Loss: {test_loss / len(test_dataloader)}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\atuli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\atuli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 44\u001b[0m, in \u001b[0;36mYourModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 44\u001b[0m     seasonal, trend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecomposition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeasonal shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, seasonal\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrend shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, trend\u001b[38;5;241m.\u001b[39mshape)        \n",
      "File \u001b[1;32mc:\\Users\\atuli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\atuli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m, in \u001b[0;36mDecompositionLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# padding on the both ends of time series\u001b[39;00m\n\u001b[0;32m     17\u001b[0m num_of_pads \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 18\u001b[0m front \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, num_of_pads, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m end \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:, :]\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, num_of_pads, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m x_padded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([front, x, end], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class SolarPowerDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        x = torch.tensor(sample.drop('Solar_Power_Consumption(Kw)').values, dtype=torch.float32)\n",
    "        y = torch.tensor(sample['Solar_Power_Consumption(Kw)'], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "dailyEnergy = pd.read_excel('dailyEnergyWithFeatures.xlsx')\n",
    "\n",
    "dailyEnergy['weekday'] = dailyEnergy['TimeReviewed'].dt.dayofweek\n",
    "dailyEnergy['day_type'] = np.zeros(len(dailyEnergy))\n",
    "holidays = ['2017-12-26', '2018-1-1', '2018-1-14', '2018-1-26', '2018-5-1', '2018-8-15', '2018-10-2', '2018-12-25', '2019-1-1', '2019-1-14', '2019-1-26', '2019-5-1']\n",
    "dailyEnergy.loc[dailyEnergy['TimeReviewed'].isin(holidays), 'day_type'] = 1\n",
    "\n",
    "df_elect = dailyEnergy[[\"Temp( C)\", \"EmployeeCount\", \"weekday\", \"day_type\", 'Solar_Power_Consumption(Kw)']]\n",
    "\n",
    "train_size = 350\n",
    "test_size = 155\n",
    "elect_train = df_elect.head(train_size)\n",
    "elect_train\n",
    "elect_test = df_elect.tail(test_size)\n",
    "\n",
    "train_dataset = SolarPowerDataset(elect_train)\n",
    "test_dataset = SolarPowerDataset(elect_test)\n",
    "\n",
    "class ForeCaster(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decomposition = DecompositionLayer(kernel_size=3)\n",
    "        self.fc = nn.Linear(4, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        seasonal, trend = self.decomposition(x)\n",
    "        print(\"Seasonal shape:\", seasonal.shape)\n",
    "        print(\"Trend shape:\", trend.shape)        \n",
    "        seasonal_trend = torch.cat((seasonal, trend), dim=1)\n",
    "        print(\"Seasonal and trend concatenated shape:\", seasonal_trend.shape)  \n",
    "        output = self.fc(seasonal_trend)\n",
    "        return output\n",
    "\n",
    "model = ForeCaster()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        print(\"Input shape:\", x_batch.shape)\n",
    "        output = model(x_batch)\n",
    "#         loss = loss_fn(output.squeeze(), y_batch)\n",
    "#         total_loss += loss.item()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     print(f\"Epoch {epoch+1}, Train Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# model.eval()\n",
    "# test_loss = 0\n",
    "# with torch.no_grad():\n",
    "#     for x_batch, y_batch in test_dataloader:\n",
    "#         print(\"Input shape:\", x_batch.shape)\n",
    "#         output = model(x_batch)\n",
    "#         loss = loss_fn(output.squeeze(), y_batch)\n",
    "#         test_loss += loss.item()\n",
    "\n",
    "# print(f\"Test Loss: {test_loss / len(test_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = XX_elect_train\n",
    "y_train = YY_elect_train\n",
    "X_test = XX_elect_test\n",
    "y_test = YY_elect_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.6):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "        self.layernorm4 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout4 = layers.Dropout(rate)\n",
    "        \n",
    "def call(self, inputs, training):\n",
    "    attn_output = self.att(inputs, inputs)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "\n",
    "    out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "    ffn_output = self.ffn(out1)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "\n",
    "    out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    ffn_output2 = self.ffn(out2)\n",
    "    ffn_output2 = self.dropout3(ffn_output2, training=training)\n",
    "\n",
    "    out3 = self.layernorm3(out2 + ffn_output2)\n",
    "\n",
    "    ffn_output3 = self.ffn(out3)\n",
    "    ffn_output3 = self.dropout4(ffn_output3, training=training)  \n",
    "\n",
    "    return self.layernorm4(out3 + ffn_output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 1\n",
    "num_heads = 6\n",
    "ff_dim = 64 # related to feed forward network\n",
    "num_blocks = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 4)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_model():\n",
    "    inputs = layers.Input(shape=(X_train.shape[1],))\n",
    "    x = layers.Reshape(target_shape=(4, 1))(inputs)\n",
    "    transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_blocks)]\n",
    "    \n",
    "    for transformer_block in transformer_blocks:\n",
    "        x = transformer_block(x, training=True)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    x = layers.Dense(90, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.6)(x)\n",
    "\n",
    "    x = layers.Dense(60, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.6)(x)\n",
    "\n",
    "    x = layers.Dense(30, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.6)(x)\n",
    "    \n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "model = transformer_model()\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 189ms/step - accuracy: 0.0151 - loss: 109857136.0000 - val_accuracy: 0.0129 - val_loss: 122957824.0000\n",
      "Epoch 2/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.0176 - loss: 111502392.0000 - val_accuracy: 0.0129 - val_loss: 122957400.0000\n",
      "Epoch 3/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0114 - loss: 114231000.0000 - val_accuracy: 0.0129 - val_loss: 122956976.0000\n",
      "Epoch 4/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.0242 - loss: 116439792.0000 - val_accuracy: 0.0129 - val_loss: 122956512.0000\n",
      "Epoch 5/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.0114 - loss: 111284416.0000 - val_accuracy: 0.0129 - val_loss: 122956096.0000\n",
      "Epoch 6/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.0227 - loss: 111289232.0000 - val_accuracy: 0.0129 - val_loss: 122955656.0000\n",
      "Epoch 7/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.0136 - loss: 111603280.0000 - val_accuracy: 0.0129 - val_loss: 122955232.0000\n",
      "Epoch 8/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.0215 - loss: 114049136.0000 - val_accuracy: 0.0129 - val_loss: 122954768.0000\n",
      "Epoch 9/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.0107 - loss: 108347976.0000 - val_accuracy: 0.0129 - val_loss: 122954352.0000\n",
      "Epoch 10/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.0142 - loss: 115499144.0000 - val_accuracy: 0.0129 - val_loss: 122953952.0000\n",
      "Epoch 11/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.0098 - loss: 116259096.0000 - val_accuracy: 0.0129 - val_loss: 122953488.0000\n",
      "Epoch 12/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.0105 - loss: 116277256.0000 - val_accuracy: 0.0129 - val_loss: 122953064.0000\n",
      "Epoch 13/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.0134 - loss: 113917608.0000 - val_accuracy: 0.0129 - val_loss: 122952632.0000\n",
      "Epoch 14/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.0117 - loss: 109608008.0000 - val_accuracy: 0.0129 - val_loss: 122952208.0000\n",
      "Epoch 15/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.0090 - loss: 112966464.0000 - val_accuracy: 0.0129 - val_loss: 122951744.0000\n",
      "Epoch 16/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0173 - loss: 113093888.0000 - val_accuracy: 0.0129 - val_loss: 122951352.0000\n",
      "Epoch 17/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0139 - loss: 115554920.0000 - val_accuracy: 0.0129 - val_loss: 122950888.0000\n",
      "Epoch 18/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.0032 - loss: 117073616.0000 - val_accuracy: 0.0129 - val_loss: 122950464.0000\n",
      "Epoch 19/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.0156 - loss: 114033184.0000 - val_accuracy: 0.0129 - val_loss: 122950056.0000\n",
      "Epoch 20/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.0108 - loss: 106152064.0000 - val_accuracy: 0.0129 - val_loss: 122949608.0000\n",
      "Epoch 21/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.0222 - loss: 115467840.0000 - val_accuracy: 0.0129 - val_loss: 122949168.0000\n",
      "Epoch 22/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.0049 - loss: 113222528.0000 - val_accuracy: 0.0129 - val_loss: 122948744.0000\n",
      "Epoch 23/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0163 - loss: 108079376.0000 - val_accuracy: 0.0000e+00 - val_loss: 122948312.0000\n",
      "Epoch 24/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.0000e+00 - loss: 115087640.0000 - val_accuracy: 0.0000e+00 - val_loss: 122947864.0000\n",
      "Epoch 25/25\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0000e+00 - loss: 116308832.0000 - val_accuracy: 0.0000e+00 - val_loss: 122947448.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=25, batch_size=16, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
