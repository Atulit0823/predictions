{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense layer BI-LSTM with double attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_7     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">37,888</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_layer_13  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,321</span> │ bidirectional_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)]  │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_layer_14  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,321</span> │ bidirectional_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)]  │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenation_layer │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ attention_layer_… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConcatenationLaye…</span> │                   │            │ attention_layer_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ concatenation_la… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_7     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m37,888\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_layer_13  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │      \u001b[38;5;34m8,321\u001b[0m │ bidirectional_7[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mAttentionLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)]  │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_layer_14  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │      \u001b[38;5;34m8,321\u001b[0m │ bidirectional_7[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mAttentionLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)]  │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenation_layer │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ attention_layer_… │\n",
       "│ (\u001b[38;5;33mConcatenationLaye…\u001b[0m │                   │            │ attention_layer_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ concatenation_la… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,787</span> (214.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m54,787\u001b[0m (214.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,787</span> (214.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m54,787\u001b[0m (214.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.W = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, encoder_output):\n",
    "        score = self.V(tf.nn.tanh(self.W(encoder_output)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class ConcatenationLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ConcatenationLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.concat(inputs, axis=1)\n",
    "\n",
    "def create_model(input_shape, lstm_units, dense_units):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))(inputs)\n",
    "    context_vector1, attention_weights1 = AttentionLayer(dense_units)(x)\n",
    "    context_vector2, attention_weights2 = AttentionLayer(dense_units)(x)\n",
    "    concatenated_context = ConcatenationLayer()([context_vector1, context_vector2])\n",
    "    outputs = tf.keras.layers.Dense(1, activation='linear')(concatenated_context)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (None, 9)  \n",
    "lstm_units = 64\n",
    "dense_units = 64\n",
    "\n",
    "model = create_model(input_shape, lstm_units, dense_units)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 505 entries, 0 to 504\n",
      "Data columns (total 6 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   TimeReviewed                 505 non-null    datetime64[ns]\n",
      " 1   Solar_Power_Consumption(Kw)  505 non-null    float64       \n",
      " 2   Temp( C)                     505 non-null    float64       \n",
      " 3   EmployeeCount                505 non-null    int64         \n",
      " 4   weekday                      505 non-null    int32         \n",
      " 5   day_type                     505 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(3), int32(1), int64(1)\n",
      "memory usage: 21.8 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atuli\\AppData\\Local\\Temp\\ipykernel_18048\\3723895554.py:4: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  dailyEnergy['day_type'][(dailyEnergy['weekday']==5)|(dailyEnergy['weekday']==6)|(dailyEnergy['TimeReviewed']=='2017-12-26')|(dailyEnergy['TimeReviewed']=='2018-1-1')|(dailyEnergy['TimeReviewed']=='2018-1-14')|(dailyEnergy['TimeReviewed']=='2018-1-26')|(dailyEnergy['TimeReviewed']=='2018-5-1')|(dailyEnergy['TimeReviewed']=='2018-8-15')|(dailyEnergy['TimeReviewed']=='2018-10-2')|(dailyEnergy['TimeReviewed']=='2018-12-25')|(dailyEnergy['TimeReviewed']=='2019-1-1')|(dailyEnergy['TimeReviewed']=='2019-1-14')|(dailyEnergy['TimeReviewed']=='2019-1-26')|(dailyEnergy['TimeReviewed']=='2019-5-1')]=1\n",
      "C:\\Users\\atuli\\AppData\\Local\\Temp\\ipykernel_18048\\3723895554.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dailyEnergy['day_type'][(dailyEnergy['weekday']==5)|(dailyEnergy['weekday']==6)|(dailyEnergy['TimeReviewed']=='2017-12-26')|(dailyEnergy['TimeReviewed']=='2018-1-1')|(dailyEnergy['TimeReviewed']=='2018-1-14')|(dailyEnergy['TimeReviewed']=='2018-1-26')|(dailyEnergy['TimeReviewed']=='2018-5-1')|(dailyEnergy['TimeReviewed']=='2018-8-15')|(dailyEnergy['TimeReviewed']=='2018-10-2')|(dailyEnergy['TimeReviewed']=='2018-12-25')|(dailyEnergy['TimeReviewed']=='2019-1-1')|(dailyEnergy['TimeReviewed']=='2019-1-14')|(dailyEnergy['TimeReviewed']=='2019-1-26')|(dailyEnergy['TimeReviewed']=='2019-5-1')]=1\n"
     ]
    }
   ],
   "source": [
    "dailyEnergy = pd.read_excel('dailyEnergyWithFeatures.xlsx')\n",
    "dailyEnergy['weekday'] = dailyEnergy['TimeReviewed'].dt.dayofweek\n",
    "dailyEnergy['day_type'] = np.zeros(len(dailyEnergy))\n",
    "dailyEnergy['day_type'][(dailyEnergy['weekday']==5)|(dailyEnergy['weekday']==6)|(dailyEnergy['TimeReviewed']=='2017-12-26')|(dailyEnergy['TimeReviewed']=='2018-1-1')|(dailyEnergy['TimeReviewed']=='2018-1-14')|(dailyEnergy['TimeReviewed']=='2018-1-26')|(dailyEnergy['TimeReviewed']=='2018-5-1')|(dailyEnergy['TimeReviewed']=='2018-8-15')|(dailyEnergy['TimeReviewed']=='2018-10-2')|(dailyEnergy['TimeReviewed']=='2018-12-25')|(dailyEnergy['TimeReviewed']=='2019-1-1')|(dailyEnergy['TimeReviewed']=='2019-1-14')|(dailyEnergy['TimeReviewed']=='2019-1-26')|(dailyEnergy['TimeReviewed']=='2019-5-1')]=1\n",
    "dailyEnergy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elect = dailyEnergy[[\"Temp( C)\", \"EmployeeCount\", \"weekday\",\"day_type\", 'Solar_Power_Consumption(Kw)']]\n",
    "\n",
    "elect_train = pd.DataFrame(data=df_elect.head(350))\n",
    "elect_test = pd.DataFrame(data=df_elect.head(155))\n",
    "\n",
    "XX_elect_train = elect_train.drop('Solar_Power_Consumption(Kw)', axis = 1).reset_index().drop('index', axis = 1)\n",
    "XX_elect_test = elect_test.drop('Solar_Power_Consumption(Kw)', axis = 1).reset_index().drop('index', axis = 1)\n",
    "\n",
    "YY_elect_train = elect_train[['Solar_Power_Consumption(Kw)']]\n",
    "YY_elect_test = elect_test[['Solar_Power_Consumption(Kw)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elect = dailyEnergy[[\"Temp( C)\", \"EmployeeCount\", \"weekday\",\"day_type\", 'Solar_Power_Consumption(Kw)']]\n",
    "\n",
    "elect_train = pd.DataFrame(data=df_elect.head(350))\n",
    "elect_test = pd.DataFrame(data=df_elect.head(155))\n",
    "\n",
    "XX_elect_train = elect_train.drop('Solar_Power_Consumption(Kw)', axis = 1).reset_index().drop('index', axis = 1)\n",
    "XX_elect_test = elect_test.drop('Solar_Power_Consumption(Kw)', axis = 1).reset_index().drop('index', axis = 1)\n",
    "\n",
    "YY_elect_train = elect_train[['Solar_Power_Consumption(Kw)']]\n",
    "YY_elect_test = elect_test[['Solar_Power_Consumption(Kw)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Attention, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)   var1(t)   var2(t)  \\\n",
      "1   0.714262   0.062762   0.521552   0.166667        1.0  0.775761  0.112971   \n",
      "2   0.775761   0.112971   0.000000   0.333333        0.0  0.769228  0.040446   \n",
      "3   0.769228   0.040446   0.000000   0.500000        0.0  0.721061  0.040446   \n",
      "4   0.721061   0.040446   0.000000   0.666667        0.0  0.446244  0.184100   \n",
      "5   0.446244   0.184100   0.000000   0.833333        1.0  0.000000  0.108787   \n",
      "\n",
      "   var3(t)   var4(t)  var5(t)  \n",
      "1    0.000  0.333333      0.0  \n",
      "2    0.000  0.500000      0.0  \n",
      "3    0.000  0.666667      0.0  \n",
      "4    0.000  0.833333      1.0  \n",
      "5    0.125  1.000000      1.0  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var3(t-1)</th>\n",
       "      <th>var4(t-1)</th>\n",
       "      <th>var5(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "      <th>var2(t)</th>\n",
       "      <th>var3(t)</th>\n",
       "      <th>var4(t)</th>\n",
       "      <th>var5(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.714262</td>\n",
       "      <td>0.062762</td>\n",
       "      <td>0.521552</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.775761</td>\n",
       "      <td>0.112971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.775761</td>\n",
       "      <td>0.112971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.769228</td>\n",
       "      <td>0.040446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.769228</td>\n",
       "      <td>0.040446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.721061</td>\n",
       "      <td>0.040446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.721061</td>\n",
       "      <td>0.040446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446244</td>\n",
       "      <td>0.184100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.446244</td>\n",
       "      <td>0.184100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108787</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.739638</td>\n",
       "      <td>0.304045</td>\n",
       "      <td>0.818966</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.872778</td>\n",
       "      <td>0.336123</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.872778</td>\n",
       "      <td>0.336123</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.512937</td>\n",
       "      <td>0.285412</td>\n",
       "      <td>0.715517</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.512937</td>\n",
       "      <td>0.285412</td>\n",
       "      <td>0.715517</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.598326</td>\n",
       "      <td>0.047414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.598326</td>\n",
       "      <td>0.047414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692707</td>\n",
       "      <td>0.353237</td>\n",
       "      <td>0.801724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.692707</td>\n",
       "      <td>0.353237</td>\n",
       "      <td>0.801724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.375281</td>\n",
       "      <td>0.437110</td>\n",
       "      <td>0.831897</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>504 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)   var1(t)  \\\n",
       "1     0.714262   0.062762   0.521552   0.166667        1.0  0.775761   \n",
       "2     0.775761   0.112971   0.000000   0.333333        0.0  0.769228   \n",
       "3     0.769228   0.040446   0.000000   0.500000        0.0  0.721061   \n",
       "4     0.721061   0.040446   0.000000   0.666667        0.0  0.446244   \n",
       "5     0.446244   0.184100   0.000000   0.833333        1.0  0.000000   \n",
       "..         ...        ...        ...        ...        ...       ...   \n",
       "500   0.739638   0.304045   0.818966   0.500000        0.0  0.872778   \n",
       "501   0.872778   0.336123   0.793103   0.666667        0.0  0.512937   \n",
       "502   0.512937   0.285412   0.715517   0.833333        1.0  0.000000   \n",
       "503   0.000000   0.598326   0.047414   1.000000        1.0  0.692707   \n",
       "504   0.692707   0.353237   0.801724   0.000000        0.0  0.375281   \n",
       "\n",
       "      var2(t)   var3(t)   var4(t)  var5(t)  \n",
       "1    0.112971  0.000000  0.333333      0.0  \n",
       "2    0.040446  0.000000  0.500000      0.0  \n",
       "3    0.040446  0.000000  0.666667      0.0  \n",
       "4    0.184100  0.000000  0.833333      1.0  \n",
       "5    0.108787  0.125000  1.000000      1.0  \n",
       "..        ...       ...       ...      ...  \n",
       "500  0.336123  0.793103  0.666667      0.0  \n",
       "501  0.285412  0.715517  0.833333      1.0  \n",
       "502  0.598326  0.047414  1.000000      1.0  \n",
       "503  0.353237  0.801724  0.000000      0.0  \n",
       "504  0.437110  0.831897  0.166667      0.0  \n",
       "\n",
       "[504 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "dataset = pd.read_excel('energy.xlsx', header=0, index_col=0)\n",
    "values = dataset[['Solar_Power_Consumption(Kw)','Temp( C)','EmployeeCount','weekday','day_type']].values\n",
    "len(values)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "print(reframed.head())\n",
    "\n",
    "reframed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = reframed.values\n",
    "\n",
    "n_train_time = 365 # training size is taken for 1 year (2017/26/12 to 2018/26/12)\n",
    "train = values[:n_train_time, :]\n",
    "test = values[n_train_time:, :]\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 - 0s - 24ms/step - accuracy: 0.9726 - loss: 0.0573 - val_accuracy: 0.9784 - val_loss: 0.0618\n",
      "Epoch 2/50\n",
      "6/6 - 0s - 10ms/step - accuracy: 0.9726 - loss: 0.0570 - val_accuracy: 0.9784 - val_loss: 0.0616\n",
      "Epoch 3/50\n",
      "6/6 - 0s - 11ms/step - accuracy: 0.9726 - loss: 0.0568 - val_accuracy: 0.9784 - val_loss: 0.0614\n",
      "Epoch 4/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9726 - loss: 0.0566 - val_accuracy: 0.9784 - val_loss: 0.0613\n",
      "Epoch 5/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9726 - loss: 0.0564 - val_accuracy: 0.9784 - val_loss: 0.0611\n",
      "Epoch 6/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9726 - loss: 0.0562 - val_accuracy: 0.9784 - val_loss: 0.0609\n",
      "Epoch 7/50\n",
      "6/6 - 0s - 11ms/step - accuracy: 0.9726 - loss: 0.0560 - val_accuracy: 0.9784 - val_loss: 0.0608\n",
      "Epoch 8/50\n",
      "6/6 - 0s - 11ms/step - accuracy: 0.9726 - loss: 0.0558 - val_accuracy: 0.9784 - val_loss: 0.0606\n",
      "Epoch 9/50\n",
      "6/6 - 0s - 11ms/step - accuracy: 0.9726 - loss: 0.0557 - val_accuracy: 0.9784 - val_loss: 0.0605\n",
      "Epoch 10/50\n",
      "6/6 - 0s - 11ms/step - accuracy: 0.9726 - loss: 0.0555 - val_accuracy: 0.9784 - val_loss: 0.0603\n",
      "Epoch 11/50\n",
      "6/6 - 0s - 10ms/step - accuracy: 0.9753 - loss: 0.0553 - val_accuracy: 0.9784 - val_loss: 0.0602\n",
      "Epoch 12/50\n",
      "6/6 - 0s - 11ms/step - accuracy: 0.9753 - loss: 0.0552 - val_accuracy: 0.9784 - val_loss: 0.0601\n",
      "Epoch 13/50\n",
      "6/6 - 0s - 11ms/step - accuracy: 0.9753 - loss: 0.0550 - val_accuracy: 0.9784 - val_loss: 0.0600\n",
      "Epoch 14/50\n",
      "6/6 - 0s - 11ms/step - accuracy: 0.9753 - loss: 0.0548 - val_accuracy: 0.9784 - val_loss: 0.0598\n",
      "Epoch 15/50\n",
      "6/6 - 0s - 11ms/step - accuracy: 0.9753 - loss: 0.0547 - val_accuracy: 0.9784 - val_loss: 0.0597\n",
      "Epoch 16/50\n",
      "6/6 - 0s - 11ms/step - accuracy: 0.9753 - loss: 0.0545 - val_accuracy: 0.9784 - val_loss: 0.0596\n",
      "Epoch 17/50\n",
      "6/6 - 0s - 11ms/step - accuracy: 0.9753 - loss: 0.0543 - val_accuracy: 0.9784 - val_loss: 0.0595\n",
      "Epoch 18/50\n",
      "6/6 - 0s - 11ms/step - accuracy: 0.9753 - loss: 0.0542 - val_accuracy: 0.9784 - val_loss: 0.0594\n",
      "Epoch 19/50\n",
      "6/6 - 0s - 12ms/step - accuracy: 0.9753 - loss: 0.0540 - val_accuracy: 0.9784 - val_loss: 0.0593\n",
      "Epoch 20/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9753 - loss: 0.0539 - val_accuracy: 0.9784 - val_loss: 0.0592\n",
      "Epoch 21/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9753 - loss: 0.0537 - val_accuracy: 0.9784 - val_loss: 0.0592\n",
      "Epoch 22/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9753 - loss: 0.0536 - val_accuracy: 0.9784 - val_loss: 0.0591\n",
      "Epoch 23/50\n",
      "6/6 - 0s - 10ms/step - accuracy: 0.9753 - loss: 0.0534 - val_accuracy: 0.9784 - val_loss: 0.0590\n",
      "Epoch 24/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9753 - loss: 0.0533 - val_accuracy: 0.9784 - val_loss: 0.0589\n",
      "Epoch 25/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9753 - loss: 0.0531 - val_accuracy: 0.9784 - val_loss: 0.0588\n",
      "Epoch 26/50\n",
      "6/6 - 0s - 10ms/step - accuracy: 0.9753 - loss: 0.0529 - val_accuracy: 0.9784 - val_loss: 0.0587\n",
      "Epoch 27/50\n",
      "6/6 - 0s - 10ms/step - accuracy: 0.9753 - loss: 0.0528 - val_accuracy: 0.9784 - val_loss: 0.0586\n",
      "Epoch 28/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9781 - loss: 0.0526 - val_accuracy: 0.9784 - val_loss: 0.0585\n",
      "Epoch 29/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9781 - loss: 0.0525 - val_accuracy: 0.9784 - val_loss: 0.0585\n",
      "Epoch 30/50\n",
      "6/6 - 0s - 10ms/step - accuracy: 0.9781 - loss: 0.0523 - val_accuracy: 0.9784 - val_loss: 0.0584\n",
      "Epoch 31/50\n",
      "6/6 - 0s - 10ms/step - accuracy: 0.9781 - loss: 0.0522 - val_accuracy: 0.9784 - val_loss: 0.0583\n",
      "Epoch 32/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9781 - loss: 0.0520 - val_accuracy: 0.9784 - val_loss: 0.0582\n",
      "Epoch 33/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9781 - loss: 0.0518 - val_accuracy: 0.9784 - val_loss: 0.0581\n",
      "Epoch 34/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9781 - loss: 0.0517 - val_accuracy: 0.9784 - val_loss: 0.0580\n",
      "Epoch 35/50\n",
      "6/6 - 0s - 10ms/step - accuracy: 0.9781 - loss: 0.0515 - val_accuracy: 0.9784 - val_loss: 0.0580\n",
      "Epoch 36/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9781 - loss: 0.0514 - val_accuracy: 0.9784 - val_loss: 0.0579\n",
      "Epoch 37/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9781 - loss: 0.0512 - val_accuracy: 0.9784 - val_loss: 0.0578\n",
      "Epoch 38/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9781 - loss: 0.0510 - val_accuracy: 0.9784 - val_loss: 0.0577\n",
      "Epoch 39/50\n",
      "6/6 - 0s - 10ms/step - accuracy: 0.9781 - loss: 0.0509 - val_accuracy: 0.9784 - val_loss: 0.0576\n",
      "Epoch 40/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9781 - loss: 0.0507 - val_accuracy: 0.9784 - val_loss: 0.0575\n",
      "Epoch 41/50\n",
      "6/6 - 0s - 10ms/step - accuracy: 0.9781 - loss: 0.0505 - val_accuracy: 0.9784 - val_loss: 0.0574\n",
      "Epoch 42/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9781 - loss: 0.0503 - val_accuracy: 0.9784 - val_loss: 0.0573\n",
      "Epoch 43/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9781 - loss: 0.0502 - val_accuracy: 0.9784 - val_loss: 0.0572\n",
      "Epoch 44/50\n",
      "6/6 - 0s - 10ms/step - accuracy: 0.9781 - loss: 0.0500 - val_accuracy: 0.9784 - val_loss: 0.0571\n",
      "Epoch 45/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9781 - loss: 0.0498 - val_accuracy: 0.9784 - val_loss: 0.0570\n",
      "Epoch 46/50\n",
      "6/6 - 0s - 11ms/step - accuracy: 0.9781 - loss: 0.0496 - val_accuracy: 0.9784 - val_loss: 0.0569\n",
      "Epoch 47/50\n",
      "6/6 - 0s - 9ms/step - accuracy: 0.9781 - loss: 0.0494 - val_accuracy: 0.9784 - val_loss: 0.0568\n",
      "Epoch 48/50\n",
      "6/6 - 0s - 10ms/step - accuracy: 0.9781 - loss: 0.0492 - val_accuracy: 0.9784 - val_loss: 0.0567\n",
      "Epoch 49/50\n",
      "6/6 - 0s - 10ms/step - accuracy: 0.9781 - loss: 0.0490 - val_accuracy: 0.9784 - val_loss: 0.0566\n",
      "Epoch 50/50\n",
      "6/6 - 0s - 10ms/step - accuracy: 0.9781 - loss: 0.0488 - val_accuracy: 0.9784 - val_loss: 0.0565\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X, train_y, epochs=50, batch_size=70, validation_data=(test_X, test_y), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
